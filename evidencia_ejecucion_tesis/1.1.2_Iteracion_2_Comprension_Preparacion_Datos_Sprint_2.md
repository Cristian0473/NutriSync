# 1.1.2. Iteración 2: Comprensión y Preparación de Datos (CRISP-DM) + Sprint 2 (SCRUM)

En esta iteración se ejecutaron las fases de Comprensión de Datos y Preparación de Datos de CRISP-DM, alineadas al Sprint 2 de SCRUM. El propósito fue definir las fuentes de datos, estandarizar formatos, instrumentar los formularios y endpoints necesarios para el registro confiable de información clínica, antropométrica y de catálogo nutricional, y consolidar un data mart relacional que respalde el modelado posterior (Sprint 3).

## 1.1.2.1. Alcance del sprint

El Sprint 2 habilitó la captura y normalización de los siguientes dominios: datos clínicos por paciente, antropometría y hábitos, catálogo de ingredientes con sus atributos nutricionales, y padrón de pacientes y perfiles de profesionales.

Todos los formularios, validaciones y persistencia se implementaron sobre Flask y el módulo de acceso a datos (fetch_one, fetch_all, execute). Los entregables fueron: pantallas administrativas para CRUD, parsers y validadores, y la integración con el esquema relacional previamente definido.

## 1.1.2.2. Fuentes de datos y mecanismos de captura

Fuentes de datos implementadas:

Variable hba1c: Tipo decimal. Origen: Clínico. Reglas: Rango esperado 0-20; opcional. Validación en UI y servidor mediante función _num que convierte strings a float con manejo de excepciones.

Variable glucosa_ayunas: Tipo decimal. Origen: Clínico. Reglas: mg/dL; opcional. Almacenamiento en columna DECIMAL de PostgreSQL para precisión numérica.

Variable ldl: Tipo decimal. Origen: Clínico. Reglas: mg/dL; opcional. Permite valores nulos para pacientes sin medición reciente.

Variable pa_sis y pa_dia: Tipo smallint. Origen: Clínico. Reglas: 60-250 para sistólica, 40-150 para diastólica. Validación de rangos clínicos razonables para prevenir errores de captura.

Variable meds_json: Tipo json/text. Origen: Clínico. Reglas: Lista normalizada como arreglo de strings. Se procesa mediante _parse_json_list que acepta JSON válido o texto separado por comas y lo convierte a lista Python, luego se serializa con json.dumps para almacenamiento.

Variable otros_json: Tipo json/text. Origen: Clínico. Reglas: Lista normalizada como arreglo de strings. Mismo procesamiento que meds_json para mantener consistencia.

Variable peso: Tipo decimal. Origen: Antropometría. Reglas: Numérico; opcional. Conversión segura mediante _num_or_none que retorna None si el valor está vacío, compatible con columnas DECIMAL NULL.

Variable talla: Tipo decimal. Origen: Antropometría. Reglas: Mayor a 0 para cálculo IMC. Validación adicional en frontend para prevenir divisiones por cero.

Variable cc: Tipo decimal. Origen: Antropometría. Reglas: Opcional. Circunferencia de cintura en centímetros.

Variable bf_pct: Tipo decimal. Origen: Antropometría. Reglas: Opcional. Porcentaje de grasa corporal, típicamente entre 10 y 50 por ciento.

Variable actividad: Tipo varchar(10). Origen: Antropometría. Reglas: Valores controlados {baja, moderada, alta}. Normalización mediante _actividad_norm que mapea variaciones de texto (ej: "Baja", "BAJA", "baja") a valor canónico.

Se desarrollaron formularios protegidos por sesión y rol mediante decoradores login_required y admin_required que garantizan trazabilidad y acceso controlado a la capa de datos.

## 1.1.2.3. Registro clínico por paciente

Rutas implementadas: GET /admin/clinico para listado, POST /admin/clinico/nuevo para creación, POST /admin/clinico/<id>/editar para actualización, POST /admin/clinico/<id>/borrar para eliminación lógica.

Variables capturadas: fecha del control, HbA1c, glucosa en ayunas, LDL, presión arterial sistólica y diastólica, medicación y otros hallazgos.

Estandarización implementada: Los valores numéricos se convierten con funciones robustas _num y _num_or_none que manejan strings vacíos, valores None y excepciones de conversión, reduciendo fallos por campos vacíos. Las listas clínicas (medicación y otros) se guardan en JSON a través de _parse_json_list y serialización con json.dumps, favoreciendo flexibilidad ante listas de longitud variable y minimizando nulls mal formados.

La función _parse_json_list implementa lógica que detecta si la entrada es JSON válido (mediante json.loads) o texto plano separado por comas o saltos de línea. Si es texto, lo divide y limpia espacios en blanco, luego lo convierte a lista Python antes de serializar a JSON para almacenamiento.

## 1.1.2.4. Antropometría y hábitos

Rutas implementadas: GET /admin/antropometria para listado, POST /admin/antropometria/nuevo para creación, POST /admin/antropometria/<id>/editar para actualización, POST /admin/antropometria/<id>/borrar para eliminación.

Variables capturadas: fecha, peso, talla, circunferencia de cintura (CC), porcentaje de grasa corporal y nivel de actividad.

Estandarización implementada: Conversión segura de numéricos con _num_or_none compatible con columnas DECIMAL que aceptan NULL. Normalización de actividad con _actividad_norm a categorías controladas: baja, moderada, alta. El IMC se calcula en la vista mediante JavaScript para fines de monitoreo y validación visual (no se persiste el derivado), disminuyendo el riesgo de inconsistencias por cálculos manuales.

La función _actividad_norm implementa un diccionario de mapeo que convierte variaciones comunes de texto a valores canónicos. Por ejemplo, "sedentario", "poco activo" se mapean a "baja"; "activo", "regular" se mapean a "moderada"; "muy activo", "intenso" se mapean a "alta".

## 1.1.2.5. Catálogo de ingredientes y atributos nutricionales

Rutas implementadas: GET /admin/ingredientes para listado, POST /admin/ingredientes/nuevo para creación, POST /admin/ingredientes/<id>/editar para actualización, POST /admin/ingredientes/<id>/toggle para activar/desactivar, POST /admin/ingredientes/<id>/borrar para eliminación lógica.

Variables capturadas: nombre, grupo alimentario, energía (kcal), macronutrientes (CHO, PRO, FAT), fibra, índice glucémico (IG), sodio, costo, unidad y porción base, y tags.

Estandarización implementada: Grupos restringidos por lista blanca ING_GRUPOS que incluye: CEREAL, PROTEINA, VERDURA, FRUTA, LACTEO, GRASA, OTRO. Posteriormente se migró a nomenclatura de Guía de Intercambio: GRUPO1_CEREALES, GRUPO2_VERDURAS, GRUPO3_FRUTAS, GRUPO4_LACTEOS, GRUPO5_CARNES, GRUPO6_AZUCARES, GRUPO7_GRASAS.

Números convertidos a DECIMAL mediante _to_decimal que maneja strings, floats e integers, retornando Decimal de Python para precisión en cálculos monetarios y nutricionales. IG y otras discretas normalizadas a int mediante casting explícito con validación de rangos.

Etiquetas en tags_json a través de _norm_tags que acepta JSON válido o texto separado por comas o saltos de línea. La función parsea la entrada, limpia espacios, elimina duplicados y retorna JSON serializado, lo que homogeneiza el etiquetado para búsquedas y filtros posteriores.

## 1.1.2.6. Padrón de pacientes y perfiles profesionales

Pacientes (/admin/pacientes): El alta y edición usan ensure_preregistro para asegurar consistencia con pre_registro, satisfaciendo FK y almacenando nombres y apellidos de ser provistos. Se contempla la asociación opcional a usuario y la garantía del rol paciente cuando se registra un correo mediante función get_or_create_user_with_paciente_role.

La función ensure_preregistro implementa lógica de UPSERT que verifica existencia de pre_registro por DNI. Si existe, actualiza campos modificables; si no existe, crea nuevo registro. Esto garantiza que todo paciente tenga un preregistro correspondiente, manteniendo integridad referencial.

Nutricionistas (/admin/nutricionistas): Se normalizan campos del perfil profesional, incluyendo horarios en JSON y validación de cadenas con _clip que limita longitud, _list_from_text que convierte texto a lista, y _norm_sexo que normaliza valores de sexo a M, F u O. La función ensure_role asegura la integridad del mapeo usuario_rol, creando el registro si no existe.

## 1.1.2.7. Reglas de validación y limpieza aplicadas

La capa de preparación implementa un conjunto de parsers y normalizadores que operan antes de la persistencia:

Numéricos: _to_decimal, _num_or_none y casts explícitos previenen excepciones y evitan valores en blanco que rompan DECIMAL/INTEGER. La función _to_decimal utiliza Decimal de Python que proporciona precisión decimal exacta, importante para cálculos nutricionales donde pequeños errores pueden acumularse.

Listas y JSON: _parse_json_list, _parse_list_or_json y _json_or_none admiten entradas flexibles (JSON válido o texto separado por comas) y las convierten a estructuras bien formadas, minimizando registros inválidos. Estas funciones implementan try-except para manejar JSON mal formado, retornando estructuras por defecto en caso de error.

Categóricos: _actividad_norm (actividad física) y _norm_sexo (M/F/O) acotan el espacio de valores, fortaleciendo la consistencia analítica. Estas funciones utilizan diccionarios de mapeo y comparación case-insensitive para normalizar variaciones de entrada.

Integridad referencial: ensure_preregistro garantiza que toda creación o edición en paciente esté respaldada por un preregistro; ensure_role mantiene la coherencia en usuario_rol. Estas funciones implementan transacciones implícitas mediante ejecución atómica de múltiples consultas SQL.

## 1.1.2.8. Modelo lógico del data mart

Durante la iteración se consolidó la porción del esquema necesaria para capturar y preparar datos:

Tabla paciente: Campos id (PK), usuario_id (FK opcional a usuario), dni (único), con FK hacia usuario y concordancia en pre_registro. La tabla almacena datos demográficos básicos y permite asociación opcional con cuenta de usuario activa.

Tabla clinico: Campos id (PK), paciente_id (FK a paciente), fecha, hba1c, glucosa_ayunas, ldl, pa_sis, pa_dia, meds_json, otros_json. Permite múltiples registros por paciente para seguimiento histórico. El campo fecha permite ordenamiento temporal para obtener último control.

Tabla antropometria: Campos id (PK), paciente_id (FK a paciente), fecha, peso, talla, cc, bf_pct, actividad. Similar a clinico, permite seguimiento histórico de medidas antropométricas.

Tabla ingrediente: Campos id (PK), nombre (único), grupo, kcal, cho, pro, fat, fibra, ig, sodio, costo, unidad_base, porcion_base, tags_json, activo. El campo activo permite desactivar ingredientes sin eliminarlos, manteniendo integridad referencial en planes históricos.

Apoyo transaccional con usuario, rol, usuario_rol, pre_registro y activacion_token para asegurar flujos seguros de alta y activación y gobierno de acceso. La tabla usuario_rol implementa relación many-to-many entre usuarios y roles, permitiendo que un usuario tenga múltiples roles simultáneamente.

## 1.1.2.9. Datasets para entrenamiento de modelos de Machine Learning

Además de la captura de datos clínicos y antropométricos del sistema, se identificaron y procesaron datasets públicos de gran escala para entrenar los modelos de machine learning que proporcionan capacidades predictivas al sistema de recomendación.

### Dataset 1: NHANES (National Health and Nutrition Examination Survey)

Fuente y obtención: NHANES es una encuesta nacional de salud y nutrición realizada por los Centros para el Control y la Prevención de Enfermedades (CDC) de Estados Unidos. Los datos son de acceso público y se descargaron desde el sitio web oficial de NHANES (https://www.cdc.gov/nchs/nhanes/). Se utilizaron datos de 4 ciclos: 2013-2014, 2015-2016, 2017-2018 y 2021-2023, descargados en formato XPT (formato SAS).

Procesamiento: Se desarrolló script procesar_nhanes_multi_anio.py que realiza las siguientes operaciones: Carga archivos XPT de cada año mediante biblioteca pyreadstat que permite leer formato SAS. Unifica variables que cambian de nombre entre años (ej: BPX y BPXO para presión arterial). Mapea variables NHANES a formato del sistema mediante diccionarios de mapeo que convierten códigos NHANES a nombres estándar. Crea variables derivadas: IMC calculado como peso dividido por talla al cuadrado, LDL calculado mediante fórmula de Friedewald cuando triglicéridos están disponibles, HOMA-IR calculado como (insulina en ayunas multiplicado por glucosa en ayunas) dividido por 405, ratios como TG/HDL y LDL/HDL, índice aterogénico (AIP) calculado como log(triglicéridos/HDL). Filtra pacientes con diabetes tipo 2 y prediabetes mediante criterios: HbA1c mayor o igual a 5.7% (prediabetes) o diagnóstico médico de diabetes. Valida rangos clínicos mediante funciones que verifican que valores estén dentro de rangos fisiológicos razonables (ej: HbA1c entre 3% y 20%, glucosa entre 50 y 500 mg/dL). Limpia datos faltantes mediante imputación de mediana para variables numéricas y eliminación de registros con más del 50% de datos faltantes en variables esenciales.

Resultado: Archivo nhanes_procesado.csv con 12,057 pacientes, 26 variables clínicas, antropométricas y derivadas. Variables incluyen: clínicas (HbA1c, glucosa en ayunas, insulina, lípidos, presión arterial), antropométricas (peso, talla, IMC, circunferencia de cintura), derivadas (HOMA-IR, ratios, índices), targets (control_glucemico binario basado en HbA1c mayor o igual a 7.0%, riesgo_metabolico continuo 0-1 calculado como combinación de múltiples factores de riesgo).

Utilidad: Este dataset se utiliza para entrenar el modelo de predicción de control glucémico que ajusta metas nutricionales según probabilidad de mal control. El modelo aprende patrones de asociación entre características clínicas y antropométricas y control glucémico, permitiendo predecir riesgo de mal control para nuevos pacientes.

### Dataset 2: CGMacros (PhysioNet)

Fuente y obtención: CGMacros es un dataset de acceso público disponible en PhysioNet (https://physionet.org/) que contiene datos de monitoreo continuo de glucosa (CGM), registro de comidas y datos bioquímicos de sujetos con diabetes. El dataset se descargó desde PhysioNet en formato comprimido y se descomprimió en directorio local.

Estructura del dataset: El dataset incluye múltiples archivos CSV, uno por cada sujeto (CGMacros-001.csv, CGMacros-002.csv, etc.), cada uno con datos temporales de: Monitoreo continuo de glucosa (CGM) con timestamps y valores de glucosa en mg/dL. Registro de comidas con tipo de comida (Breakfast, Lunch, Dinner), macronutrientes (calorías, carbohidratos, proteínas, grasas, fibra) y cantidad consumida. Actividad física con frecuencia cardíaca (HR) y calorías de actividad. Archivo bio.csv con datos bioquímicos de cada sujeto: edad, sexo, IMC, peso, talla, HbA1c, glucosa en ayunas, insulina, lípidos (triglicéridos, colesterol, HDL, LDL), HOMA-IR.

Procesamiento: Se desarrolló script procesar_cgmacros.py que realiza las siguientes operaciones: Carga archivo bio.csv con datos bioquímicos de todos los sujetos. Itera sobre todos los archivos CGMacros-*.csv y carga datos de cada sujeto. Combina datos bioquímicos con datos temporales mediante merge por subject_id. Procesa timestamps y extrae fecha, hora y día de la semana. Limpia y convierte valores numéricos mediante pd.to_numeric con manejo de errores. Crea columna de glucosa unificada que prioriza Dexcom GL sobre Libre GL (ambos son sensores CGM). Calcula tiempo desde última comida mediante diferencia de timestamps entre comidas consecutivas. Consolida todos los datos en un único DataFrame con columnas estandarizadas.

Resultado: Archivo cgmacros_procesado.csv con datos temporales de múltiples sujetos, incluyendo perfiles bioquímicos, registros de comidas y respuestas glucémicas postprandiales. Este dataset se utiliza como base para preparar datos específicos para el Modelo 1 (respuesta glucémica) mediante script preparar_datos_modelo1_respuesta_glucemica.py que calcula métricas de respuesta glucémica (incremento, pico, tiempo hasta pico) para cada comida.

Utilidad: Este dataset se utiliza para entrenar el Modelo 1 que predice respuesta glucémica postprandial. El modelo aprende a predecir cómo responderá la glucosa de un paciente específico a un alimento específico, permitiendo excluir alimentos que causarían picos glucémicos excesivos antes de incluirlos en el plan.

### Dataset 3: MyFitnessPal (Kaggle)

Fuente y obtención: MyFitnessPal es un dataset público disponible en Kaggle (https://www.kaggle.com/) que contiene registros de alimentos consumidos por usuarios de la aplicación MyFitnessPal. El dataset se descargó desde Kaggle en formato TSV (Tab-Separated Values).

Estructura del dataset: El archivo mfp-diaries.tsv contiene registros diarios de usuarios con estructura: user_id: identificador único del usuario, date: fecha del registro, meals_json: JSON con información de comidas del día, incluyendo nombre de comida (Breakfast, Lunch, Dinner), platos (dishes) con nombre de alimento y valores nutricionales (calorías, carbohidratos, proteínas, grasas, sodio, azúcar), totals_json: JSON con totales nutricionales del día y objetivos.

Procesamiento: Se desarrolló script procesar_mfp.py que realiza las siguientes operaciones: Lee archivo TSV en chunks de 10,000 filas para manejar tamaño del archivo (varios gigabytes). Parsea JSON anidado en columnas meals_json y totals_json mediante json.loads. Extrae información estructurada de cada comida: tipo de comida (Breakfast, Lunch, Dinner), nombre del alimento, valores nutricionales (calorías, carbohidratos, proteínas, grasas, sodio, azúcar). Normaliza nombres de alimentos mediante limpieza de strings (remoción de caracteres especiales, normalización de mayúsculas/minúsculas). Agrupa alimentos por nombre y calcula valores nutricionales promedio para alimentos con múltiples registros. Filtra alimentos con frecuencia mínima (al menos 10 consumos) para reducir ruido. Limita a top 10,000 alimentos más frecuentes para mantener tamaño manejable del dataset.

Resultado: Archivo mfp_procesado.csv con alimentos únicos, valores nutricionales promedio y frecuencia de consumo. Este dataset se combina con CGMacros mediante script preparar_datos_modelo2_seleccion_alimentos.py para crear dataset de entrenamiento del Modelo 2 que calcula score de idoneidad de alimentos.

Utilidad: Este dataset se utiliza para entrenar el Modelo 2 que calcula score de idoneidad de alimentos. El modelo aprende a identificar qué alimentos son más adecuados para pacientes con diferentes perfiles clínicos, basándose en frecuencia de consumo (indicador de aceptabilidad) y respuesta glucémica estimada.

### Preparación de datos específicos para cada modelo

Modelo 1: Predicción de Respuesta Glucémica: Script preparar_datos_modelo1_respuesta_glucemica.py procesa cgmacros_procesado.csv para crear modelo1_respuesta_glucemica.csv. Para cada comida registrada, calcula respuesta glucémica postprandial: glucose_baseline (promedio de glucosa 30 minutos antes de la comida), glucose_peak (pico de glucosa en ventana de 3 horas después de la comida), glucose_increment (diferencia entre pico y baseline), time_to_peak (tiempo en minutos desde la comida hasta el pico), glucose_2h (glucosa aproximada a las 2 horas), glucose_auc (área bajo la curva de glucosa postprandial). Combina perfil del paciente (edad, sexo, IMC, datos bioquímicos) con características del alimento (kcal, CHO, PRO, FAT, fibra) y contexto (hora, tipo de comida, tiempo desde última comida). Resultado: dataset con features de paciente y alimento como input, y métricas de respuesta glucémica como targets.

Modelo 2: Selección Personalizada de Alimentos: Script preparar_datos_modelo2_seleccion_alimentos.py combina mfp_procesado.csv y cgmacros_procesado.csv. Para cada alimento de MyFitnessPal, busca perfiles nutricionales similares en CGMacros y calcula respuesta glucémica promedio. Calcula score de idoneidad (0-1) basado en: respuesta glucémica (50% del score: menor incremento = mejor), frecuencia de consumo (30% del score: más consumo = mejor, indica preferencia), preferencia explícita si está disponible (20% del score). Crea registros para cada combinación alimento-perfil de paciente. Resultado: dataset con features de paciente y alimento como input, y score de idoneidad como target.

Modelo 3: Optimización de Combinaciones: Script preparar_datos_modelo3_combinaciones.py procesa cgmacros_procesado.csv para crear modelo3_combinaciones.csv. Agrupa comidas del mismo día que están cercanas en tiempo (dentro de 1 hora) para representar "comidas completas" con varios platos. Calcula características agregadas de la combinación: totales nutricionales (kcal, CHO, PRO, FAT, fibra), proporciones (porcentaje de cada macronutriente), diversidad (número de tipos de comida diferentes). Calcula respuesta glucémica total de la combinación mediante función calcular_respuesta_combinacion que analiza ventana de 3 horas desde la primera comida. Calcula score de calidad (0-1) basado en incremento de glucosa (70% del score) y variabilidad de glucosa (30% del score). Resultado: dataset con features de paciente y combinación de alimentos como input, y score de calidad como target.

### Estadísticas de los datasets procesados

Dataset NHANES: 12,057 pacientes, 26 variables, 4 años de datos (2013-2023), completitud variable (LDL 17.1%, triglicéridos 17.4%, glucosa en ayunas 62.8%, peso y talla 99.9%).

Dataset CGMacros procesado: Múltiples sujetos con datos temporales, miles de comidas registradas con respuestas glucémicas medidas, datos bioquímicos completos para cada sujeto.

Dataset MyFitnessPal procesado: Top 10,000 alimentos más frecuentes, valores nutricionales promedio calculados, frecuencia de consumo por alimento.

### Justificación de la selección de datasets

NHANES proporciona muestra representativa a nivel nacional de pacientes con diabetes, con datos clínicos estandarizados y validados. CGMacros proporciona datos reales de respuesta glucémica postprandial medida mediante CGM, permitiendo entrenar modelos predictivos precisos. MyFitnessPal proporciona datos de consumo real de alimentos por usuarios, indicando aceptabilidad y preferencias. La combinación de estos tres datasets permite entrenar modelos que combinan conocimiento clínico (NHANES), respuesta fisiológica medida (CGMacros) y aceptabilidad práctica (MyFitnessPal).

## 1.1.2.10. Integración y preparación para el análisis

Con los formularios y validaciones en marcha, se habilitó la extracción consistente para análisis exploratorio (EDA) y features de primer nivel:

Derivados inmediatos como IMC calculado a partir de peso y talla en consultas SQL o en el frontend mediante JavaScript. El cálculo se realiza dinámicamente para evitar inconsistencias por actualizaciones parciales de datos.

Construcción de vistas o consultas que integran último control clínico más última antropometría por paciente para snapshots de entrenamiento. Estas consultas utilizan subconsultas correlacionadas con MAX(fecha) para obtener el registro más reciente de cada tabla por paciente.

Homogeneización de fechas en formato YYYY-MM-DD mediante funciones de PostgreSQL TO_CHAR y casting explícito, y estandarización de unidades (porción/unidad base en ingredientes) mediante normalización de strings y conversión a unidades canónicas (gramos, mililitros, unidades).

La preparación de datos estableció las bases para el modelado posterior, garantizando que los datos ingresados al sistema sean consistentes, completos y adecuados para entrenamiento de modelos de machine learning. Los datasets públicos procesados proporcionan la base de conocimiento necesaria para entrenar modelos ML que mejoran la personalización y precisión de las recomendaciones nutricionales.

